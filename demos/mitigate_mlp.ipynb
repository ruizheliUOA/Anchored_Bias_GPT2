{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from fancy_einsum import einsum\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from rich import print as rprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "print(\"Disabled automatic differentiation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_small_model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "# Get the default device used\n",
    "device: t.device = utils.get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../gpt2_small_data/IOI.json\", \"r\") as f:\n",
    "    data_samples = json.load(f)\n",
    "\n",
    "anchored_data = [data['sentence'] for data in data_samples[:1]]\n",
    "answers = [(\" \" + data_samples[i][\"label\"], \" A\") for i in range(len(data_samples[:1]))]\n",
    "\n",
    "rprint(anchored_data[0])\n",
    "rprint(answers[0])\n",
    "answer_tokens = t.concat([\n",
    "    gpt2_small_model.to_tokens(names, prepend_bos=False).T for names in answers\n",
    "])\n",
    "rprint(answer_tokens[0])\n",
    "print(len(data_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = gpt2_small_model.to_tokens(anchored_data[0], prepend_bos=True)\n",
    "original_logits, gpt2_cache = gpt2_small_model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_ls = [i for i in range(gpt2_small_model.cfg.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_lens_detector(model, model_cache, anchored_pos, correct_pos, layer_index):\n",
    "    sent_weighted_values = model_cache[\"mlp_out\", layer_index]\n",
    "    correct_pos_mlp_lens = einsum(\"i,i->\", model.W_U[:, correct_pos], model.ln_final(sent_weighted_values[0, -1, :])) + model.b_U[correct_pos] # logit lens for correct pos\n",
    "    anchored_pos_mlp_lens = einsum(\"i,i->\", model.W_U[:, anchored_pos], model.ln_final(sent_weighted_values[0, -1, :])) + model.b_U[anchored_pos] # logit lens for anchored pos\n",
    "    print(f\"Correct pos mlp lens: {correct_pos_mlp_lens}\")\n",
    "    print(f\"Anchored pos mlp lens: {anchored_pos_mlp_lens}\")\n",
    "    \n",
    "    return (anchored_pos_mlp_lens - correct_pos_mlp_lens).item(), correct_pos_mlp_lens.item(), anchored_pos_mlp_lens.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_dim_contribution_detector(model, model_cache, anchored_pos, correct_pos, layer_index, gap_threshold=4):\n",
    "    coefficients_final_pos = model_cache[\"post\", layer_index].squeeze(0)[-1]\n",
    "    W_out = model.W_out[layer_index]\n",
    "\n",
    "    contributions_ls = []\n",
    "    for i in range(coefficients_final_pos.shape[0]):\n",
    "        coefficient_abs = t.abs(coefficients_final_pos[i])\n",
    "        value_2_norm = t.linalg.vector_norm(W_out[i], ord=2)\n",
    "        contributions_ls.append(coefficient_abs * value_2_norm)\n",
    "\n",
    "    contributions = t.stack(contributions_ls)\n",
    "\n",
    "    topk_contributions, topk_indices = t.topk(contributions, k=10, largest=True)\n",
    "\n",
    "    topk_correct_pos_contributions = [] # abs value of correct pos contribution\n",
    "    for i in range(topk_indices.shape[0]):\n",
    "        correct_pos_contribution = einsum(\"d_model, d_model->\", model.W_U[:, correct_pos], model.ln_final(coefficients_final_pos[topk_indices[i]] * W_out[topk_indices[i]] + model.b_out[layer_index])) + model.b_U[correct_pos]\n",
    "        topk_correct_pos_contributions.append(correct_pos_contribution)\n",
    "\n",
    "    topk_anchor_pos_contributions = [] # abs value of anchored pos contribution\n",
    "    for i in range(topk_indices.shape[0]):\n",
    "        anchor_pos_contribution = einsum(\"d_model, d_model->\", model.W_U[:, anchored_pos], model.ln_final(coefficients_final_pos[topk_indices[i]] * W_out[topk_indices[i]] + model.b_out[layer_index])) + model.b_U[anchored_pos]\n",
    "        topk_anchor_pos_contributions.append(anchor_pos_contribution)\n",
    "\n",
    "    topk_words_ls = [] # visualize top k words stored in model's W_out based on specific dimension\n",
    "    for i in range(topk_indices.shape[0]):\n",
    "        per_value_words = []\n",
    "        value_unembed = einsum(\"d_model, d_model d_vocab-> d_vocab\", model.ln_final(W_out[topk_indices[i]] + model.b_out[layer_index]), model.W_U) + model.b_U\n",
    "        prob_unembed = value_unembed.softmax(dim=-1)\n",
    "        prob_unembed_values, prob_unembed_indices = prob_unembed.sort(descending=True)\n",
    "        for j in range(20):\n",
    "            per_value_words.append(model.to_string(prob_unembed_indices[j]))\n",
    "        \n",
    "        topk_words_ls.append(per_value_words)\n",
    "\n",
    "    diff_contributions = t.stack(topk_anchor_pos_contributions) - t.stack(topk_correct_pos_contributions)\n",
    "    large_gap_indices = t.where(diff_contributions > gap_threshold)[0]\n",
    "    large_gap_topk_indices = topk_indices[large_gap_indices]\n",
    "\n",
    "    large_gap_topk_words_ls = [topk_words_ls[i] for i in large_gap_indices]\n",
    "    \n",
    "    return large_gap_topk_indices, diff_contributions[large_gap_indices].tolist(), large_gap_topk_words_ls, topk_contributions.tolist(), topk_indices.tolist(), topk_correct_pos_contributions, topk_anchor_pos_contributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_mlp(model, gap_dim, layer_index, anchored_pos, correct_pos, alpha_1 = 1, alpha_2 = 8): # follow Value_dim = Value_dim - alpha_1 * anchord_unemebed + alpha_2 * correct_unembed\n",
    "    original_W_out = model.blocks[layer_index].mlp.W_out.clone()\n",
    "    model.blocks[layer_index].mlp.W_out[gap_dim, :] = model.blocks[layer_index].mlp.W_out[gap_dim, :] - alpha_1 * model.W_U[:, anchored_pos] + alpha_2 * model.W_U[:, correct_pos]\n",
    "    return original_W_out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_compare(modified_model, tokens, original_logits, correct_pos):\n",
    "    modified_logits, modified_gpt2_cache = modified_model.run_with_cache(tokens)\n",
    "    modified_final_pos_probs = modified_logits.squeeze(0)[-1].softmax(dim=-1)\n",
    "    modified_final_pos_values, modified_final_pos_indices = modified_final_pos_probs.sort(descending=True)\n",
    "    \n",
    "    \n",
    "    top_20_words_modified = []\n",
    "    for i in range(20):\n",
    "        top_20_words_modified.append(modified_model.to_string(modified_final_pos_indices[i]))\n",
    "    modified_correct_prob = modified_final_pos_probs[correct_pos]\n",
    "    \n",
    "\n",
    "    original_final_pos_probs = original_logits.squeeze(0)[-1].softmax(dim=-1)\n",
    "    original_final_pos_values, original_final_pos_indices = original_final_pos_probs.sort(descending=True)\n",
    "    top_20_words_original = []\n",
    "    for i in range(20):\n",
    "        top_20_words_original.append(modified_model.to_string(original_final_pos_indices[i]))\n",
    "    \n",
    "    original_correct_prob = original_final_pos_probs[correct_pos]\n",
    "\n",
    "    return modified_final_pos_indices[0].item() == correct_pos, modified_final_pos_values[0], original_final_pos_values[0], top_20_words_modified, top_20_words_original, modified_correct_prob, original_correct_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sent: {gpt2_small_model.to_string(tokens)}\")\n",
    "\n",
    "for layer_index in layers_ls[::-1]:\n",
    "    \n",
    "    print(f\"Layer: {layer_index}\")\n",
    "    \n",
    "    mlp_lens, correct_pos_lens, anchored_pos_lens = mlp_lens_detector(gpt2_small_model, gpt2_cache, answer_tokens[0][1].item(), answer_tokens[0][0].item(), layer_index)\n",
    "\n",
    "    print(f\"MLP logit difference at layer {layer_index}: {mlp_lens}\")\n",
    "\n",
    "    large_gap_topk_indices, diff_contributions, large_gap_topk_words_ls, topk_contributions, topk_indices, took_correct_pos_contributions, topk_anchor_pos_contributions = coefficient_dim_contribution_detector(gpt2_small_model, gpt2_cache, answer_tokens[0][1].item(), answer_tokens[0][0].item(), layer_index)\n",
    "\n",
    "    print(f\"Large diff dimensions at layer {layer_index} are {[i.item() for i in large_gap_topk_indices]}\")\n",
    "    print(f\"Top 20 words for large diff dimensions at layer {layer_index} are {large_gap_topk_words_ls}\")\n",
    "    print(f\"Diff logit contributions for large diff dimensions at layer {layer_index} are {diff_contributions}\")\n",
    "\n",
    "    if len(large_gap_topk_indices) != 0:\n",
    "                \n",
    "        original_W_out, modified_model = fix_mlp(gpt2_small_model, large_gap_topk_indices[0], layer_index, answer_tokens[0][1].item(), answer_tokens[0][0].item())\n",
    "        \n",
    "        prediction_result, modified_final_pos_top1_value, original_final_pos_top1_value, top_20_words_modified, top_20_words_original, modified_correct_prob, original_correct_prob = prediction_compare(modified_model, tokens.squeeze(0), original_logits, answer_tokens[0][0].item())\n",
    "        \n",
    "\n",
    "        print(f\"Modified prediction result is {prediction_result}, modified next token prob is {modified_final_pos_top1_value}, original next token prob is {original_final_pos_top1_value}\")\n",
    "        print(f\"Top 20 words for modified model are {top_20_words_modified}\")\n",
    "        print(f\"Top 20 words for original model are {top_20_words_original}\")\n",
    "        print(\"--------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        gpt2_small_model.blocks[layer_index].mlp.W_out[large_gap_topk_indices[0], :] = original_W_out[large_gap_topk_indices[0], :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MI_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
